{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5ee1fe8-cd48-48ce-9589-5cf4ad1d1421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Requirement already satisfied: nltk in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385530a7-40a7-45cf-8cef-c9364326b2de",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84eedcd8-c8b4-44cd-abdd-759d58def7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/61cd9919-c0e9-4c1d-a277-\n",
      "[nltk_data]     256313933197/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # This line downloads the 'punkt' resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62d86b5-84ea-4ed8-8f34-bd7d7bd1e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus= ''' Hello welcome to sanjay fartyal's youtube channel. \n",
    "Here we will learn about devil. \n",
    "Please like share and subscribe.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e994d0e3-3a81-419a-b855-a7cf5685e570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello welcome to sanjay fartyal's youtube channel. \n",
      "Here we will learn about devil. \n",
      "Please like share and subscribe.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a508158-4c55-4e8d-ac3c-de929ed33bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "## Convert sentence into paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8640c58-2a4b-4d56-ae09-6d4debfa19f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b09d69d2-8532-45cc-99f8-40f39be37d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'sanjay',\n",
       " 'fartyal',\n",
       " \"'s\",\n",
       " 'youtube',\n",
       " 'channel',\n",
       " '.',\n",
       " 'Here',\n",
       " 'we',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'devil',\n",
       " '.',\n",
       " 'Please',\n",
       " 'like',\n",
       " 'share',\n",
       " 'and',\n",
       " 'subscribe',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1601d97-a86d-4c61-86ab-ac3c7820ca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "document= sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "214a3998-7526-402f-99d7-e02aa2c0718d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'welcome', 'to', 'sanjay', 'fartyal', \"'s\", 'youtube', 'channel', '.']\n",
      "['Here', 'we', 'will', 'learn', 'about', 'devil', '.']\n",
      "['Please', 'like', 'share', 'and', 'subscribe', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in document:\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f410a49-f366-4f04-9307-d1d05e91048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f40d8b81-c182-4e7b-9516-59a0651f46c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'sanjay',\n",
       " 'fartyal',\n",
       " \"'\",\n",
       " 's',\n",
       " 'youtube',\n",
       " 'channel',\n",
       " '.',\n",
       " 'Here',\n",
       " 'we',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'devil',\n",
       " '.',\n",
       " 'Please',\n",
       " 'like',\n",
       " 'share',\n",
       " 'and',\n",
       " 'subscribe',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bba15cea-b220-4043-8d9b-d44c8d713f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4776b508-42cf-4204-be14-4c190109f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer= TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffd9c7dc-4013-417d-9b6b-3f2506e64966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'sanjay',\n",
       " 'fartyal',\n",
       " \"'s\",\n",
       " 'youtube',\n",
       " 'channel.',\n",
       " 'Here',\n",
       " 'we',\n",
       " 'will',\n",
       " 'learn',\n",
       " 'about',\n",
       " 'devil.',\n",
       " 'Please',\n",
       " 'like',\n",
       " 'share',\n",
       " 'and',\n",
       " 'subscribe',\n",
       " '.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcbdee1-9056-46b2-8f8a-13142914b5a1",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c1008f-6fe4-49e6-a48c-1d7bb08f77e8",
   "metadata": {},
   "source": [
    "- Stemming is the process of reducing the word to its word stem, that affixes or suffixes or to the root of the word known as llemma.\n",
    "- Like eating eat eaten--> eat and go gone goes --> go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "145f167c-f96f-4911-87b1-8a1a7058fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['eating','eaten','eaten','programmming','program','writing','written', 'history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afd446f5-0690-445d-a62c-e1ffe06354e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7a6527d-78a8-4ae5-ad11-c7107c529394",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9c96185-8642-4b82-a388-8a93ab0dce20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eaten---->eaten\n",
      "eaten---->eaten\n",
      "programmming---->programm\n",
      "program---->program\n",
      "writing---->write\n",
      "written---->written\n",
      "history---->histori\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+'---->'+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e32d19e1-d825-4959-bd6f-82853a46474c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'congratul'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('congratulations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99bf0eb5-2fb4-45fb-9696-8c6687a3d891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'modif'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('modification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c3210-9af9-41c1-bb7b-68804cc7c09b",
   "metadata": {},
   "source": [
    "## RegEx stemmer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ecbbe10c-04ee-4a82-b96e-4f2cafb0cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d7f7d87-0719-480e-9e55-0c5a790e76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_stem= RegexpStemmer('ing$|s$|s$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9607befe-cf18-4145-8be1-7be4062812ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_stem.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e2a5de67-7b28-4d3d-9c3a-8d1660211643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingeat'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_stem.stem('ingeating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a4c9aaba-5630-4867-92ca-a2b7196b0d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_stem.stem('callable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b9d9a-aec5-4de0-8e2a-1fe2a3f474da",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_stem.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "03c37aa3-d032-403a-a15b-a89cb8c7c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_stem1= RegexpStemmer('ing|s$|s$|able$', min=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "62b4c033-d573-4f83-946c-8cddffa96f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_stem1.stem('ingeating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d1ff46-4983-42fa-b5fe-89fbda3e4fcf",
   "metadata": {},
   "source": [
    "## Snowball stemmer\n",
    "- Gives better accuracy than porterstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6eabd615-9f5e-49ce-bb8b-ba147c059daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bc880576-eb74-437a-9bd6-b7ed47f949e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_stemmer= SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "afe5bf01-2292-4cec-bc87-2d7ac0e9e1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating---->eat\n",
      "eaten---->eaten\n",
      "eaten---->eaten\n",
      "programmming---->programm\n",
      "program---->program\n",
      "writing---->write\n",
      "written---->written\n",
      "history---->histori\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+'---->'+snow_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de035497-f643-42c4-9d96-1063166b3e83",
   "metadata": {},
   "source": [
    "### Porter stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a472e330-21bb-4605-80da-9c61324dd423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming.stem('fairly'), stemming.stem('sportingly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e45a519-3067-4e3a-8d95-fa2875463e7b",
   "metadata": {},
   "source": [
    "### Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "32f99a8e-5703-4f0f-abdb-d2cec9528e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snow_stemmer.stem('fairly'), snow_stemmer.stem('sportingly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac641096-197f-469c-87c1-60f6e1942760",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "- Lemma means root word\n",
    "- Main aim of the lemmatizer is to give exact root word for the words\n",
    "- After lemmatization we will get a valid words\n",
    "- It works with the help of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "17b453c9-b7f8-4a8f-a99b-bccc1f8cfcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5fa44cdc-0d1a-48e3-a62e-1c09a5f276ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemma= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b3e4750e-7555-46e0-96a0-7ac6096de0b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/61cd9919-c0e9-4c1d-a277-256313933197/nltk_data'\n    - '/opt/conda/envs/anaconda-panel-2023.05-py310/nltk_data'\n    - '/opt/conda/envs/anaconda-panel-2023.05-py310/share/nltk_data'\n    - '/opt/conda/envs/anaconda-panel-2023.05-py310/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/home/61cd9919-c0e9-4c1d-a277-256313933197/nltk_data'\n    - '/opt/conda/envs/anaconda-panel-2023.05-py310/nltk_data'\n    - '/opt/conda/envs/anaconda-panel-2023.05-py310/share/nltk_data'\n    - '/opt/conda/envs/anaconda-panel-2023.05-py310/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wordnet_lemma\u001b[38;5;241m.\u001b[39mlemmatize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/nltk/stem/wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m wn\u001b[38;5;241m.\u001b[39m_morphy(word, pos)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-panel-2023.05-py310/lib/python3.11/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/home/61cd9919-c0e9-4c1d-a277-256313933197/nltk_data'\n    - '/opt/conda/envs/anaconda-panel-2023.05-py310/nltk_data'\n    - '/opt/conda/envs/anaconda-panel-2023.05-py310/share/nltk_data'\n    - '/opt/conda/envs/anaconda-panel-2023.05-py310/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "wordnet_lemma.lemmatize(\"going\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a9a96-8cac-4ecc-b477-5e9378862f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb1f0c-0e58-4520-8282-5336a5e38219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3627c35b-2d1e-40d7-8170-2e19ac1034ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fdb9f3-dc85-4482-9309-d0e8f5afdbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
